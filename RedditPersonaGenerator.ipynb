{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef5db7e-a8c8-4623-842d-98a16b2663d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Using cached groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (0.28.1)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Using cached groq-0.30.0-py3-none-any.whl (131 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, python-dotenv, pydantic-core, distro, annotated-types, pydantic, groq\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [python-dotenv]\n",
      "   ---------------------- ----------------- 4/7 [annotated-types]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------- ----------- 5/7 [pydantic]\n",
      "   ---------------------------------- ----- 6/7 [groq]\n",
      "   ---------------------------------- ----- 6/7 [groq]\n",
      "   ---------------------------------- ----- 6/7 [groq]\n",
      "   ---------------------------------------- 7/7 [groq]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 groq-0.30.0 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq requests python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79051f5-1057-4643-9e8c-4194b761b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb09903-e311-4080-aa56-08098c350338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Pillow\n",
      "  Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.1/7.0 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/7.0 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 19.8 MB/s eta 0:00:00\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-11.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f42fa2c-9c0f-4c2c-a7d0-501a51bebc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing API connections...\n",
      "‚úÖ Reddit API connected successfully\n",
      "‚úÖ Groq API connected successfully\n",
      "Reddit User Persona Generator Pipeline\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste Reddit profile URLs (comma-separated):\n",
      " https://www.reddit.com/user/VR2005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 URL(s) to process\n",
      "\n",
      "Processing 1/1: https://www.reddit.com/user/VR2005\n",
      "\n",
      "============================================================\n",
      " Processing: https://www.reddit.com/user/VR2005\n",
      "============================================================\n",
      "Username extracted: VR2005\n",
      "Fetching data for u/VR2005...\n",
      "‚úÖ User found: VR2005 (ID: i8klxes3)\n",
      "üìù Fetching posts...\n",
      "Fetching comments...\n",
      "‚úÖ Fetched 50 posts and 50 comments\n",
      "Generating persona with Groq AI...\n",
      "‚úÖ Persona generated successfully\n",
      "‚úÖ Persona saved to: VR2005_persona.txt\n",
      "Creating enhanced persona card with complete content for VR2005...\n",
      "Extracting complete persona content from VR2005_persona.txt...\n",
      "‚úÖ Successfully extracted complete persona content\n",
      "Downloading avatar for VR2005...\n",
      "‚ö†Ô∏è Could not download avatar: name 'BytesIO' is not defined\n",
      "‚úÖ Enhanced persona card with complete content saved: VR2005_persona_card.png\n",
      "‚úÖ Successfully processed VR2005\n",
      "\n",
      "============================================================\n",
      "PIPELINE RESULTS\n",
      "============================================================\n",
      "‚úÖ VR2005: 50 posts, 50 comments\n",
      " Files: VR2005_persona.txt, VR2005_persona_card.png\n",
      "\n",
      " Pipeline error: 'avatar_file'\n"
     ]
    }
   ],
   "source": [
    "# Reddit User Persona Generator Pipeline\n",
    "# Based on your existing code with improvements and added features\n",
    "\n",
    "# Required installations:\n",
    "# pip install praw groq requests python-dotenv pillow\n",
    "\n",
    "import praw\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class RedditPersonaGenerator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Reddit Persona Generator with API connections\"\"\"\n",
    "        # Connect to Reddit\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "            client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "            user_agent=os.getenv(\"REDDIT_USER_AGENT\")\n",
    "        )\n",
    "        \n",
    "        # Groq API configuration\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        self.groq_api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        \n",
    "        # Test API connections\n",
    "        self.test_connections()\n",
    "    \n",
    "    def test_connections(self):\n",
    "        \"\"\"Test API connections\"\"\"\n",
    "        print(\"üîß Testing API connections...\")\n",
    "        \n",
    "        # Test Reddit connection\n",
    "        try:\n",
    "            self.reddit.user.me()\n",
    "            print(\"‚úÖ Reddit API connected successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Reddit API connection failed: {e}\")\n",
    "        \n",
    "        # Test Groq connection\n",
    "        try:\n",
    "            headers = {\"Authorization\": f\"Bearer {self.groq_api_key}\"}\n",
    "            response = requests.get(\"https://api.groq.com/openai/v1/models\", headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Groq API connected successfully\")\n",
    "            else:\n",
    "                print(f\"‚ùå Groq API connection failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Groq API connection failed: {e}\")\n",
    "    \n",
    "    def extract_username_from_url(self, url: str) -> str:\n",
    "        \"\"\"Extract username from Reddit profile URL\"\"\"\n",
    "        # Handle various Reddit URL formats\n",
    "        url = url.strip().rstrip('/')\n",
    "        \n",
    "        # Remove protocol if present\n",
    "        if url.startswith('http'):\n",
    "            url = url.split('://', 1)[1]\n",
    "        \n",
    "        # Extract username using regex patterns\n",
    "        patterns = [\n",
    "            r'reddit\\.com/user/([^/]+)',\n",
    "            r'reddit\\.com/u/([^/]+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        \n",
    "        # Fallback: split by '/' and take the last part\n",
    "        parts = url.split('/')\n",
    "        if len(parts) > 0:\n",
    "            return parts[-1]\n",
    "        \n",
    "        return url\n",
    "    \n",
    "    def fetch_user_data(self, username: str, limit: int = 100) -> Dict:\n",
    "        \"\"\"\n",
    "        Fetch user data from Reddit with enhanced error handling and metadata\n",
    "        \n",
    "        Args:\n",
    "            username: Reddit username\n",
    "            limit: Maximum number of posts/comments to fetch\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing user data\n",
    "        \"\"\"\n",
    "        print(f\"Fetching data for u/{username}...\")\n",
    "        \n",
    "        try:\n",
    "            user = self.reddit.redditor(username)\n",
    "            \n",
    "            # Check if user exists by accessing id\n",
    "            try:\n",
    "                user_id = user.id\n",
    "                print(f\"‚úÖ User found: {username} (ID: {user_id})\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå User {username} not found or suspended: {e}\")\n",
    "                return {\n",
    "                    'username': username,\n",
    "                    'posts': [],\n",
    "                    'comments': [],\n",
    "                    'avatar_url': None,\n",
    "                    'error': f'User not found or suspended: {e}'\n",
    "                }\n",
    "            \n",
    "            user_data = {\n",
    "                'username': username,\n",
    "                'posts': [],\n",
    "                'comments': [],\n",
    "                'avatar_url': None,\n",
    "                'created_utc': None,\n",
    "                'karma': {\n",
    "                    'post_karma': 0,\n",
    "                    'comment_karma': 0\n",
    "                },\n",
    "                'subreddit_activity': {}\n",
    "            }\n",
    "            \n",
    "            # Get user metadata\n",
    "            try:\n",
    "                user_data['created_utc'] = user.created_utc\n",
    "                user_data['karma']['post_karma'] = user.link_karma\n",
    "                user_data['karma']['comment_karma'] = user.comment_karma\n",
    "                \n",
    "                # Try to get avatar URL\n",
    "                if hasattr(user, 'icon_img') and user.icon_img:\n",
    "                    user_data['avatar_url'] = user.icon_img.replace('&amp;', '&')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not fetch user metadata: {e}\")\n",
    "            \n",
    "            # Fetch posts\n",
    "            print(f\"üìù Fetching posts...\")\n",
    "            post_count = 0\n",
    "            try:\n",
    "                for submission in user.submissions.new(limit=limit//2):\n",
    "                    if post_count >= limit//2:\n",
    "                        break\n",
    "                    \n",
    "                    post_data = {\n",
    "                        'id': submission.id,\n",
    "                        'type': 'post',\n",
    "                        'title': submission.title,\n",
    "                        'content': submission.selftext,\n",
    "                        'subreddit': submission.subreddit.display_name,\n",
    "                        'score': submission.score,\n",
    "                        'created_utc': submission.created_utc,\n",
    "                        'url': submission.url,\n",
    "                        'formatted_text': f\"Post: {submission.title}\\n{submission.selftext}\"\n",
    "                    }\n",
    "                    \n",
    "                    user_data['posts'].append(post_data)\n",
    "                    \n",
    "                    # Track subreddit activity\n",
    "                    subreddit = submission.subreddit.display_name\n",
    "                    user_data['subreddit_activity'][subreddit] = user_data['subreddit_activity'].get(subreddit, 0) + 1\n",
    "                    \n",
    "                    post_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error fetching posts: {e}\")\n",
    "            \n",
    "            # Fetch comments\n",
    "            print(f\"Fetching comments...\")\n",
    "            comment_count = 0\n",
    "            try:\n",
    "                for comment in user.comments.new(limit=limit//2):\n",
    "                    if comment_count >= limit//2:\n",
    "                        break\n",
    "                    \n",
    "                    comment_data = {\n",
    "                        'id': comment.id,\n",
    "                        'type': 'comment',\n",
    "                        'content': comment.body,\n",
    "                        'subreddit': comment.subreddit.display_name,\n",
    "                        'score': comment.score,\n",
    "                        'created_utc': comment.created_utc,\n",
    "                        'parent_id': comment.parent_id,\n",
    "                        'formatted_text': f\"Comment: {comment.body}\"\n",
    "                    }\n",
    "                    \n",
    "                    user_data['comments'].append(comment_data)\n",
    "                    \n",
    "                    # Track subreddit activity\n",
    "                    subreddit = comment.subreddit.display_name\n",
    "                    user_data['subreddit_activity'][subreddit] = user_data['subreddit_activity'].get(subreddit, 0) + 1\n",
    "                    \n",
    "                    comment_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error fetching comments: {e}\")\n",
    "            \n",
    "            print(f\"‚úÖ Fetched {len(user_data['posts'])} posts and {len(user_data['comments'])} comments\")\n",
    "            return user_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching user data for {username}: {e}\")\n",
    "            return {\n",
    "                'username': username,\n",
    "                'posts': [],\n",
    "                'comments': [],\n",
    "                'avatar_url': None,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def generate_persona(self, user_data: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Generate user persona using Groq API\n",
    "        \n",
    "        Args:\n",
    "            user_data: Dictionary containing user's posts and comments\n",
    "            \n",
    "        Returns:\n",
    "            Generated persona text\n",
    "        \"\"\"\n",
    "        print(\"Generating persona with Groq AI...\")\n",
    "        \n",
    "        # Prepare content for analysis\n",
    "        all_content = []\n",
    "        \n",
    "        # Add posts with metadata\n",
    "        for post in user_data['posts']:\n",
    "            content = f\"POST (r/{post['subreddit']}, Score: {post['score']}): {post['title']}\\n{post['content']}\"\n",
    "            all_content.append(content)\n",
    "        \n",
    "        # Add comments with metadata\n",
    "        for comment in user_data['comments']:\n",
    "            content = f\"COMMENT (r/{comment['subreddit']}, Score: {comment['score']}): {comment['content']}\"\n",
    "            all_content.append(content)\n",
    "        \n",
    "        # Combine content\n",
    "        combined_content = \"\\n\\n\".join(all_content)\n",
    "        \n",
    "        # Truncate if too long (Groq has token limits)\n",
    "        if len(combined_content) > 20000:\n",
    "            combined_content = combined_content[:20000] + \"\\n\\n[Content truncated due to length...]\"\n",
    "        \n",
    "        # Get top subreddits for context\n",
    "        top_subreddits = sorted(user_data['subreddit_activity'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        subreddit_context = \", \".join([f\"r/{sub} ({count} posts)\" for sub, count in top_subreddits])\n",
    "        \n",
    "        # Create enhanced prompt\n",
    "        prompt = f\"\"\"You are a user research expert. Based on the following Reddit posts and comments from user '{user_data['username']}', generate a detailed user persona.\n",
    "\n",
    "USER CONTEXT:\n",
    "- Username: {user_data['username']}\n",
    "- Total Posts: {len(user_data['posts'])}\n",
    "- Total Comments: {len(user_data['comments'])}\n",
    "- Post Karma: {user_data['karma']['post_karma']}\n",
    "- Comment Karma: {user_data['karma']['comment_karma']}\n",
    "- Most Active Subreddits: {subreddit_context}\n",
    "\n",
    "REDDIT CONTENT:\n",
    "{combined_content}\n",
    "\n",
    "Generate a detailed user persona in this EXACT format:\n",
    "\n",
    "Name: [Name or nickname based on username ]\n",
    "Age: [Estimated age range with reasoning based on interests, language, and references]\n",
    "Occupation: [Likely profession or field based on posts, interests, and expertise shown]\n",
    "Interests: [Main interests and hobbies with specific examples from posts]\n",
    "Motivations: [What drives this person based on their posts and comments]\n",
    "Habits: [Observable patterns in behavior, posting times, or discussion topics]\n",
    "Frustrations: [Issues they commonly complain about or express concern over]\n",
    "Personality: [Personality traits evident from communication style and opinions]\n",
    "Goals: [Apparent goals, aspirations, or things they're working toward]\n",
    "\n",
    "For each section, include specific citations in the format [Post/Comment ID: brief excerpt] to support your analysis. Use actual post/comment IDs from the data provided.\n",
    "\n",
    "Be thorough and analytical. Base everything on concrete evidence from their posts and comments.\"\"\"\n",
    "        \n",
    "        # Make API call to Groq\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.groq_api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": \"llama3-70b-8192\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.groq_api_url, headers=headers, json=payload)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                persona_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                print(\"‚úÖ Persona generated successfully\")\n",
    "                return persona_text\n",
    "            else:\n",
    "                print(f\"‚ùå Groq API Error {response.status_code}: {response.text}\")\n",
    "                return self.generate_fallback_persona(user_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating persona: {e}\")\n",
    "            return self.generate_fallback_persona(user_data)\n",
    "    \n",
    "    def generate_fallback_persona(self, user_data: Dict) -> str:\n",
    "        \"\"\"Generate a basic persona without AI when API fails\"\"\"\n",
    "        print(\"Generating fallback persona...\")\n",
    "        \n",
    "        username = user_data['username']\n",
    "        posts = user_data['posts']\n",
    "        comments = user_data['comments']\n",
    "        top_subreddits = sorted(user_data['subreddit_activity'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        \n",
    "        persona = f\"\"\"Name: {username}\n",
    "Age: Unable to determine from available data\n",
    "Occupation: Unable to determine from available data\n",
    "Interests: Active in {len(user_data['subreddit_activity'])} subreddits, primarily: {', '.join([f\"r/{sub} ({count} posts)\" for sub, count in top_subreddits])}\n",
    "Motivations: Regular engagement with Reddit community discussions across diverse topics\n",
    "Habits: Posted {len(posts)} posts and {len(comments)} comments, shows consistent Reddit activity\n",
    "Frustrations: Unable to determine specific frustrations from available data\n",
    "Personality: Active Reddit user with {user_data['karma']['post_karma']} post karma and {user_data['karma']['comment_karma']} comment karma\n",
    "Goals: Community participation and knowledge sharing through Reddit discussions\n",
    "\"\"\"\n",
    "        \n",
    "        return persona\n",
    "    \n",
    "    def save_to_file(self, persona_text: str, username: str) -> str:\n",
    "        \"\"\"\n",
    "        Save persona to text file with enhanced formatting\n",
    "        \n",
    "        Args:\n",
    "            persona_text: Generated persona text\n",
    "            username: Reddit username\n",
    "            \n",
    "        Returns:\n",
    "            Filename of saved file\n",
    "        \"\"\"\n",
    "        filename = f\"{username}_persona.txt\"\n",
    "        \n",
    "        # Create header with metadata\n",
    "        header = f\"\"\"Reddit User Persona Analysis\n",
    "{'='*50}\n",
    "Username: u/{username}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Generated by: Reddit Persona Generator Pipeline\n",
    "{'='*50}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(header + persona_text)\n",
    "        \n",
    "        print(f\"‚úÖ Persona saved to: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "# Modified sections for the RedditPersonaGenerator class\n",
    "\n",
    "    def download_avatar(self, avatar_url: str, username: str) -> Optional[Image.Image]:\n",
    "        \"\"\"Download and return avatar image object instead of saving to file\"\"\"\n",
    "        if not avatar_url:\n",
    "            return None\n",
    "        \n",
    "        print(f\"Downloading avatar for {username}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(avatar_url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                avatar_img = Image.open(BytesIO(response.content))\n",
    "                print(f\"‚úÖ Avatar downloaded successfully\")\n",
    "                return avatar_img\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Could not download avatar: HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not download avatar: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_complete_persona_content(self, persona_file: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract ALL content from persona file in structured format\n",
    "        \n",
    "        Args:\n",
    "            persona_file: Path to persona text file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all structured content\n",
    "        \"\"\"\n",
    "        print(f\"Extracting complete persona content from {persona_file}...\")\n",
    "        \n",
    "        try:\n",
    "            with open(persona_file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract header information\n",
    "            header_info = {}\n",
    "            username_match = re.search(r'Username:\\s*(.+)', content)\n",
    "            if username_match:\n",
    "                header_info['username'] = username_match.group(1).strip()\n",
    "            \n",
    "            date_match = re.search(r'Generated:\\s*(.+)', content)\n",
    "            if date_match:\n",
    "                header_info['generated_date'] = date_match.group(1).strip()\n",
    "            \n",
    "            # Extract user context stats\n",
    "            stats_info = {}\n",
    "            stats_patterns = {\n",
    "                'total_posts': r'Total Posts:\\s*(\\d+)',\n",
    "                'total_comments': r'Total Comments:\\s*(\\d+)',\n",
    "                'post_karma': r'Post Karma:\\s*(\\d+)',\n",
    "                'comment_karma': r'Comment Karma:\\s*(\\d+)',\n",
    "                'most_active_subreddits': r'Most Active Subreddits:\\s*(.+)'\n",
    "            }\n",
    "            \n",
    "            for key, pattern in stats_patterns.items():\n",
    "                match = re.search(pattern, content)\n",
    "                if match:\n",
    "                    if key == 'most_active_subreddits':\n",
    "                        stats_info[key] = match.group(1).strip()\n",
    "                    else:\n",
    "                        stats_info[key] = int(match.group(1))\n",
    "            \n",
    "            # Extract persona sections with complete content\n",
    "            persona_sections = {}\n",
    "            \n",
    "            # Find the start of persona content (after \"REDDIT CONTENT:\" or similar)\n",
    "            persona_start = content.find('Generate a detailed user persona')\n",
    "            if persona_start == -1:\n",
    "                persona_start = content.find('Name:')\n",
    "            \n",
    "            if persona_start != -1:\n",
    "                persona_content = content[persona_start:]\n",
    "                \n",
    "                # Define section patterns to capture everything\n",
    "                section_patterns = {\n",
    "                    'Name': r'Name:\\s*([^:]+?)(?=Age:|$)',\n",
    "                    'Age': r'Age:\\s*([^:]+?)(?=Occupation:|$)',\n",
    "                    'Occupation': r'Occupation:\\s*([^:]+?)(?=Interests:|$)',\n",
    "                    'Interests': r'Interests:\\s*([^:]+?)(?=Motivations:|$)',\n",
    "                    'Motivations': r'Motivations:\\s*([^:]+?)(?=Habits:|$)',\n",
    "                    'Habits': r'Habits:\\s*([^:]+?)(?=Frustrations:|$)',\n",
    "                    'Frustrations': r'Frustrations:\\s*([^:]+?)(?=Personality:|$)',\n",
    "                    'Personality': r'Personality:\\s*([^:]+?)(?=Goals:|$)',\n",
    "                    'Goals': r'Goals:\\s*(.+?)(?=\\n\\n|$)'\n",
    "                }\n",
    "                \n",
    "                for section_name, pattern in section_patterns.items():\n",
    "                    match = re.search(pattern, persona_content, re.DOTALL)\n",
    "                    if match:\n",
    "                        content_text = match.group(1).strip()\n",
    "                        # Clean up the content\n",
    "                        content_text = re.sub(r'\\n+', ' ', content_text)  # Replace newlines with spaces\n",
    "                        content_text = re.sub(r'\\s+', ' ', content_text)  # Normalize spaces\n",
    "                        persona_sections[section_name] = content_text\n",
    "                    else:\n",
    "                        persona_sections[section_name] = 'Not specified'\n",
    "            \n",
    "            result = {\n",
    "                'header_info': header_info,\n",
    "                'stats_info': stats_info,\n",
    "                'persona_sections': persona_sections\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ Successfully extracted complete persona content\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting persona content: {e}\")\n",
    "            return {\n",
    "                'header_info': {},\n",
    "                'stats_info': {},\n",
    "                'persona_sections': {}\n",
    "            }\n",
    "    \n",
    "    def create_enhanced_persona_card(self, persona_file: str, username: str, avatar_url: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Create a comprehensive persona card with ALL content from persona file\n",
    "        \n",
    "        Args:\n",
    "            persona_file: Path to persona text file\n",
    "            username: Reddit username\n",
    "            avatar_url: Avatar URL (not file path)\n",
    "            \n",
    "        Returns:\n",
    "            Filename of generated persona card\n",
    "        \"\"\"\n",
    "        print(f\"Creating enhanced persona card with complete content for {username}...\")\n",
    "        \n",
    "        # Extract complete persona content\n",
    "        persona_data = self.extract_complete_persona_content(persona_file)\n",
    "        \n",
    "        # Download avatar as image object\n",
    "        avatar_img = None\n",
    "        if avatar_url:\n",
    "            avatar_img = self.download_avatar(avatar_url, username)\n",
    "        \n",
    "        # Enhanced card dimensions for more content\n",
    "        card_width = 1600\n",
    "        card_height = 1200\n",
    "        \n",
    "        # Create image with modern gradient background\n",
    "        img = Image.new('RGB', (card_width, card_height), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Create sophisticated gradient background\n",
    "        for y in range(card_height):\n",
    "            ratio = y / card_height\n",
    "            r = int(10 + ratio * 30)   # Dark navy to medium blue\n",
    "            g = int(15 + ratio * 40)   \n",
    "            b = int(35 + ratio * 80)   \n",
    "            draw.line([(0, y), (card_width, y)], fill=(r, g, b))\n",
    "        \n",
    "        # Load fonts with better sizing\n",
    "        try:\n",
    "            title_font = ImageFont.truetype(\"arial.ttf\", 36)\n",
    "            section_font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "            content_font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "            username_font = ImageFont.truetype(\"arial.ttf\", 28)\n",
    "            stats_font = ImageFont.truetype(\"arial.ttf\", 13)\n",
    "            small_font = ImageFont.truetype(\"arial.ttf\", 11)\n",
    "        except:\n",
    "            try:\n",
    "                title_font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 36)\n",
    "                section_font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", 16)\n",
    "                content_font = ImageFont.truetype(\"DejaVuSans.ttf\", 12)\n",
    "                username_font = ImageFont.truetype(\"DejaVuSans.ttf\", 28)\n",
    "                stats_font = ImageFont.truetype(\"DejaVuSans.ttf\", 13)\n",
    "                small_font = ImageFont.truetype(\"DejaVuSans.ttf\", 11)\n",
    "            except:\n",
    "                title_font = ImageFont.load_default()\n",
    "                section_font = ImageFont.load_default()\n",
    "                content_font = ImageFont.load_default()\n",
    "                username_font = ImageFont.load_default()\n",
    "                stats_font = ImageFont.load_default()\n",
    "                small_font = ImageFont.load_default()\n",
    "        \n",
    "        # Header section with avatar integration\n",
    "        header_height = 140\n",
    "        \n",
    "        # Draw header background\n",
    "        for y in range(header_height):\n",
    "            ratio = y / header_height\n",
    "            r = int(20 + ratio * 15)\n",
    "            g = int(25 + ratio * 20)\n",
    "            b = int(45 + ratio * 25)\n",
    "            draw.line([(0, y), (card_width, y)], fill=(r, g, b))\n",
    "        \n",
    "        # Avatar integration in header\n",
    "        avatar_size = 80\n",
    "        avatar_x = 50\n",
    "        avatar_y = 30\n",
    "        \n",
    "        if avatar_img:\n",
    "            try:\n",
    "                # Resize and create circular avatar\n",
    "                avatar_resized = avatar_img.resize((avatar_size, avatar_size))\n",
    "                \n",
    "                # Create circular mask\n",
    "                mask = Image.new('L', (avatar_size, avatar_size), 0)\n",
    "                draw_mask = ImageDraw.Draw(mask)\n",
    "                draw_mask.ellipse((0, 0, avatar_size, avatar_size), fill=255)\n",
    "                \n",
    "                # Apply mask\n",
    "                avatar_resized.putalpha(mask)\n",
    "                img.paste(avatar_resized, (avatar_x, avatar_y), avatar_resized)\n",
    "                \n",
    "                # Add border around avatar\n",
    "                draw.ellipse([avatar_x-2, avatar_y-2, avatar_x + avatar_size+2, avatar_y + avatar_size+2], \n",
    "                            outline=(255, 255, 255), width=3)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not process avatar: {e}\")\n",
    "                # Draw placeholder\n",
    "                draw.ellipse([avatar_x, avatar_y, avatar_x + avatar_size, avatar_y + avatar_size], \n",
    "                            fill=(100, 100, 100), outline=(255, 255, 255), width=2)\n",
    "                draw.text((avatar_x + 25, avatar_y + 30), \"üë§\", fill='white', font=username_font)\n",
    "        else:\n",
    "            # Draw placeholder\n",
    "            draw.ellipse([avatar_x, avatar_y, avatar_x + avatar_size, avatar_y + avatar_size], \n",
    "                        fill=(100, 100, 100), outline=(255, 255, 255), width=2)\n",
    "            draw.text((avatar_x + 25, avatar_y + 30), \"üë§\", fill='white', font=username_font)\n",
    "        \n",
    "        # Title and username next to avatar\n",
    "        title_start_x = avatar_x + avatar_size + 30\n",
    "        title_text = \"Reddit User Persona Analysis\"\n",
    "        draw.text((title_start_x, avatar_y + 5), title_text, fill='white', font=title_font)\n",
    "        \n",
    "        username_text = f\"u/{username}\"\n",
    "        draw.text((title_start_x, avatar_y + 45), username_text, fill='#64B5F6', font=username_font)\n",
    "        \n",
    "        # Generation date\n",
    "        header_info = persona_data.get('header_info', {})\n",
    "        date_text = f\"Generated: {header_info.get('generated_date', 'Unknown')}\"\n",
    "        draw.text((title_start_x, avatar_y + 80), date_text, fill='#B0BEC5', font=stats_font)\n",
    "        \n",
    "        # Stats section - comprehensive display\n",
    "        stats_y = header_height + 10\n",
    "        stats_height = 80\n",
    "        \n",
    "        # Draw stats background\n",
    "        draw.rectangle([20, stats_y, card_width - 20, stats_y + stats_height], \n",
    "                      fill=(25, 35, 55), outline=(70, 80, 100), width=2)\n",
    "        \n",
    "        # Display comprehensive stats\n",
    "        stats_info = persona_data.get('stats_info', {})\n",
    "        if stats_info:\n",
    "            # Line 1: Post and Comment stats\n",
    "            stats_line1 = f\"üìä Total Posts: {stats_info.get('total_posts', 'N/A')} | Total Comments: {stats_info.get('total_comments', 'N/A')}\"\n",
    "            draw.text((40, stats_y + 15), stats_line1, fill='#FFD700', font=stats_font)\n",
    "            \n",
    "            # Line 2: Karma stats\n",
    "            stats_line2 = f\"‚≠ê Post Karma: {stats_info.get('post_karma', 'N/A')} | Comment Karma: {stats_info.get('comment_karma', 'N/A')}\"\n",
    "            draw.text((40, stats_y + 35), stats_line2, fill='#64B5F6', font=stats_font)\n",
    "            \n",
    "            # Line 3: Subreddit activity\n",
    "            subreddit_text = stats_info.get('most_active_subreddits', 'N/A')\n",
    "            if len(subreddit_text) > 100:\n",
    "                subreddit_text = subreddit_text[:100] + \"...\"\n",
    "            draw.text((40, stats_y + 55), f\"üèõÔ∏è Most Active: {subreddit_text}\", fill='#81C784', font=small_font)\n",
    "        \n",
    "        # Content area - Enhanced 3-column layout with complete content\n",
    "        content_start_y = stats_y + stats_height + 20\n",
    "        content_height = card_height - content_start_y - 60\n",
    "        \n",
    "        # Column configuration\n",
    "        col_width = (card_width - 160) // 3\n",
    "        col_margin = 50\n",
    "        \n",
    "        # Organize sections into 3 columns with better distribution\n",
    "        persona_sections = persona_data.get('persona_sections', {})\n",
    "        \n",
    "        column_sections = [\n",
    "            ['Name', 'Age', 'Occupation'],\n",
    "            ['Interests', 'Motivations', 'Habits'],\n",
    "            ['Frustrations', 'Personality', 'Goals']\n",
    "        ]\n",
    "        \n",
    "        # Enhanced section icons\n",
    "        section_icons = {\n",
    "            'Name': 'üë§',\n",
    "            'Age': 'üéÇ',\n",
    "            'Occupation': 'üíº',\n",
    "            'Interests': 'üéØ',\n",
    "            'Motivations': 'üí°',\n",
    "            'Habits': 'üîÑ',\n",
    "            'Frustrations': 'üò§',\n",
    "            'Personality': 'üß†',\n",
    "            'Goals': 'üéØ'\n",
    "        }\n",
    "        \n",
    "        # Draw enhanced columns with complete content\n",
    "        for col_idx, sections in enumerate(column_sections):\n",
    "            col_x = col_margin + col_idx * (col_width + col_margin)\n",
    "            \n",
    "            # Draw column background with gradient effect\n",
    "            for y in range(content_start_y - 15, card_height - 80):\n",
    "                ratio = (y - content_start_y) / content_height\n",
    "                r = int(35 + ratio * 10)\n",
    "                g = int(45 + ratio * 15)\n",
    "                b = int(65 + ratio * 20)\n",
    "                draw.line([(col_x - 15, y), (col_x + col_width + 15, y)], fill=(r, g, b))\n",
    "            \n",
    "            # Column border\n",
    "            draw.rectangle([col_x - 15, content_start_y - 15, col_x + col_width + 15, card_height - 80], \n",
    "                         outline=(80, 90, 110), width=2)\n",
    "            \n",
    "            current_y = content_start_y + 10\n",
    "            \n",
    "            for section in sections:\n",
    "                # Check if we have space for this section\n",
    "                if current_y + 100 > card_height - 100:\n",
    "                    break\n",
    "                    \n",
    "                # Section header with icon\n",
    "                icon = section_icons.get(section, 'üìã')\n",
    "                header_text = f\"{icon} {section}\"\n",
    "                draw.text((col_x, current_y), header_text, fill='#FFD700', font=section_font)\n",
    "                current_y += 25\n",
    "                \n",
    "                # Section content - get complete content\n",
    "                content = persona_sections.get(section, 'Not specified')\n",
    "                \n",
    "                # Clean content but preserve important information\n",
    "                content = re.sub(r'\\[([^\\]]+)\\]', r'(\\1)', content)  # Convert citations to parentheses\n",
    "                content = ' '.join(content.split())  # Clean up whitespace\n",
    "                \n",
    "                # Enhanced text wrapping with better content fitting\n",
    "                wrapped_lines = []\n",
    "                words = content.split()\n",
    "                current_line = \"\"\n",
    "                \n",
    "                for word in words:\n",
    "                    test_line = current_line + (\" \" if current_line else \"\") + word\n",
    "                    bbox = draw.textbbox((0, 0), test_line, font=content_font)\n",
    "                    text_width = bbox[2] - bbox[0]\n",
    "                    \n",
    "                    if text_width <= col_width - 25:\n",
    "                        current_line = test_line\n",
    "                    else:\n",
    "                        if current_line:\n",
    "                            wrapped_lines.append(current_line)\n",
    "                            current_line = word\n",
    "                        else:\n",
    "                            # Handle very long words\n",
    "                            if len(word) > 30:\n",
    "                                word = word[:30] + \"...\"\n",
    "                            wrapped_lines.append(word)\n",
    "                            current_line = \"\"\n",
    "                \n",
    "                if current_line:\n",
    "                    wrapped_lines.append(current_line)\n",
    "                \n",
    "                # Calculate available space for this section\n",
    "                space_for_section = min(150, (card_height - 100 - current_y) // len(sections) if len(sections) > 0 else 150)\n",
    "                max_lines = max(3, space_for_section // 18)\n",
    "                \n",
    "                # Draw wrapped text with optimal spacing\n",
    "                for i, line in enumerate(wrapped_lines[:max_lines]):\n",
    "                    if current_y + 18 > card_height - 100:\n",
    "                        break\n",
    "                    draw.text((col_x + 5, current_y), line, fill='white', font=content_font)\n",
    "                    current_y += 18\n",
    "                \n",
    "                # Add continuation indicator if content was truncated\n",
    "                if len(wrapped_lines) > max_lines:\n",
    "                    draw.text((col_x + 5, current_y), \"...\", fill='#888888', font=content_font)\n",
    "                    current_y += 18\n",
    "                \n",
    "                current_y += 15  # Space between sections\n",
    "        \n",
    "        # Enhanced footer\n",
    "        footer_y = card_height - 25\n",
    "        footer_text = f\"Generated by Reddit Persona Generator Pipeline | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "        footer_bbox = draw.textbbox((0, 0), footer_text, font=small_font)\n",
    "        footer_width = footer_bbox[2] - footer_bbox[0]\n",
    "        draw.text((card_width//2 - footer_width//2, footer_y), footer_text, fill='#888888', font=small_font)\n",
    "        \n",
    "        # Save the enhanced card\n",
    "        card_filename = f\"{username}_persona_card.png\"\n",
    "        img.save(card_filename)\n",
    "        \n",
    "        print(f\"‚úÖ Enhanced persona card with complete content saved: {card_filename}\")\n",
    "        return card_filename\n",
    "    \n",
    "    def process_user(self, url_or_username: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single user through the entire pipeline - MODIFIED VERSION\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" Processing: {url_or_username}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract username from URL\n",
    "        username = self.extract_username_from_url(url_or_username)\n",
    "        print(f\"Username extracted: {username}\")\n",
    "        \n",
    "        # Step 1: Fetch user data\n",
    "        user_data = self.fetch_user_data(username)\n",
    "        \n",
    "        if 'error' in user_data:\n",
    "            return {\n",
    "                'username': username,\n",
    "                'success': False,\n",
    "                'error': user_data['error']\n",
    "            }\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        total_content = len(user_data['posts']) + len(user_data['comments'])\n",
    "        if total_content == 0:\n",
    "            return {\n",
    "                'username': username,\n",
    "                'success': False,\n",
    "                'error': 'No posts or comments found'\n",
    "            }\n",
    "        \n",
    "        # Step 2: Generate persona\n",
    "        persona_text = self.generate_persona(user_data)\n",
    "        \n",
    "        # Step 3: Save persona to file\n",
    "        persona_file = self.save_to_file(persona_text, username)\n",
    "        \n",
    "        # Step 4: Create enhanced persona card (no separate avatar file)\n",
    "        card_file = self.create_enhanced_persona_card(persona_file, username, user_data.get('avatar_url'))\n",
    "        \n",
    "        result = {\n",
    "            'username': username,\n",
    "            'success': True,\n",
    "            'persona_file': persona_file,\n",
    "            'card_file': card_file,\n",
    "            'avatar_url': user_data.get('avatar_url'),  # Keep URL for reference\n",
    "            'stats': {\n",
    "                'posts': len(user_data['posts']),\n",
    "                'comments': len(user_data['comments']),\n",
    "                'post_karma': user_data['karma']['post_karma'],\n",
    "                'comment_karma': user_data['karma']['comment_karma'],\n",
    "                'subreddits': len(user_data['subreddit_activity'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Successfully processed {username}\")\n",
    "        return result\n",
    "        \n",
    "    def run_pipeline(self):\n",
    "            \"\"\"Main pipeline execution\"\"\"\n",
    "            print(\"Reddit User Persona Generator Pipeline\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Get user input\n",
    "            input_urls = input(\"Paste Reddit profile URLs (comma-separated):\\n\").strip()\n",
    "            \n",
    "            if not input_urls:\n",
    "                print(\"‚ùå No URLs provided. Exiting.\")\n",
    "                return\n",
    "            \n",
    "            # Parse URLs\n",
    "            urls = [url.strip() for url in input_urls.split(\",\") if url.strip()]\n",
    "            \n",
    "            if not urls:\n",
    "                print(\"‚ùå No valid URLs found. Exiting.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(urls)} URL(s) to process\")\n",
    "            \n",
    "            results = []\n",
    "            \n",
    "            # Process each user\n",
    "            for i, url in enumerate(urls, 1):\n",
    "                print(f\"\\nProcessing {i}/{len(urls)}: {url}\")\n",
    "                \n",
    "                try:\n",
    "                    result = self.process_user(url)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Add delay between requests to avoid rate limiting\n",
    "                    if i < len(urls):\n",
    "                        print(\"Waiting 3 seconds before next request...\")\n",
    "                        time.sleep(3)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error processing {url}: {e}\")\n",
    "                    results.append({\n",
    "                        'username': url,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            \n",
    "            # Display final results\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"PIPELINE RESULTS\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            \n",
    "            for result in results:\n",
    "                if result['success']:\n",
    "                    successful += 1\n",
    "                    stats = result['stats']\n",
    "                    print(f\"‚úÖ {result['username']}: {stats['posts']} posts, {stats['comments']} comments\")\n",
    "                    print(f\" Files: {result['persona_file']}, {result['card_file']}\")\n",
    "                    if result['avatar_file']:\n",
    "                        print(f\"  Avatar: {result['avatar_file']}\")\n",
    "                else:\n",
    "                    failed += 1\n",
    "                    print(f\"‚ùå {result['username']}: {result['error']}\")\n",
    "            \n",
    "            print(f\"\\n Summary: {successful} successful, {failed} failed\")\n",
    "            print(\" Pipeline completed!\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Initialize and run pipeline\n",
    "    try:\n",
    "        generator = RedditPersonaGenerator()\n",
    "        generator.run_pipeline()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Pipeline interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Pipeline error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4d79b-fd78-409b-a603-96b569f9bb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
